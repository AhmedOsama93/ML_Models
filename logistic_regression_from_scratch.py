# -*- coding: utf-8 -*-
"""logistic_regression_from_scratch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M2_U75bv2cByj5Ns9RKQprTFZTJSpCvn
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plot
from sklearn import linear_model
import seaborn as sns
import math
from sklearn.model_selection import train_test_split
import copy

data = pd.read_csv('customer_data.csv')

data.info()

data.head()

# shuffling data
data = data.sample(frac=1)

data.head()

getColor = lambda x: 'red' if x == 0 else 'blue'
color = list(map(getColor, data['purchased']))

data.plot(kind='scatter', x='age', y='salary', c=color)
plot.show()

data.plot(kind='scatter', x='age', y='purchased')
plot.show()

data.plot(kind='scatter', x='salary', y='purchased')
plot.show()

# splitting data
x = data.drop(columns = ['purchased']).to_numpy().reshape((-1,2))
y = data['purchased'].to_numpy()

x_train, x_test, y_train, y_test = train_test_split(x, y)

'''
x_train = x[0:320]
y_train = y[0:320]
x_test = x[320:]
y_test = y[320:]'''

x_train.shape

x_test.shape

def zscore_norm(X):
    mu = np.mean(X, axis=0)
    sigma = np.std(X, axis=0)
    x_norm = (X - mu) / sigma      

    return (x_norm, mu, sigma)

fig,ax=plot.subplots(1, 1, figsize=(12, 3))
ax.scatter(x_train[:,0], x_train[:,1])
ax.set_xlabel("age"); ax.set_ylabel("salary");
ax.set_title(r"original")
plot.axis('equal')
plot.show()

plot.scatter(x=x_train[:,0], y=x_train[:,1])
plot.show()

#normalize data using z-score
x_norm, X_mu, X_sigma = zscore_norm(x_train)
x_train = x_norm

plot.scatter(x=x_norm[:,0], y=x_norm[:,1])
plot.show()

# UNQ_C1
# GRADED FUNCTION: sigmoid

def sigmoid(z):
    return 1/(1+np.exp(-z))

sigmoid(0)

def compute_cost(X, y, w, b, lambda_= 1):
    m, n = X.shape
    cost = 0.0
    
    for i in range(m):
        z = np.dot(X[i],w) + b
        f_wb = sigmoid(z)
        cost += -y[i] * np.log(f_wb) - (1-y[i])*np.log(1-f_wb)
    total_cost = cost/m
    return total_cost

m, n = x_train.shape

# Compute and display cost with w initialized to zeroes
initial_w = np.zeros(n)
initial_b = 0.
cost = compute_cost(x_train, y_train, initial_w, initial_b)
print('Cost at initial w (zeros): {:.3f}'.format(cost))

w = np.array([0.2, 0.2])
b = 0.5


cost = compute_cost(x_train , y_train, w, b)

print('Cost at test w,b: {:.3f}'.format(cost))

def compute_gradient(X, y, w, b, lambda_=None): 
    m, n = X.shape
    dj_dw = np.zeros(w.shape)
    dj_db = 0.
    for i in range(m):
        f_wb_i = sigmoid(np.dot(X[i],w) + b)          
        err_i  = f_wb_i  - y[i]                       
        for j in range(n):
            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      
        dj_db = dj_db + err_i
    dj_dw = dj_dw/m                                   
    dj_db = dj_db/m                                   
        
    return dj_db, dj_dw

def gradient_descent(X, y, w_in, b_in, alpha, iters): 
    jList = []
    w = copy.deepcopy(w_in)
    b = b_in
    
    for i in range(iters):
        # Calculate the gradient and update the parameters
        dj_db, dj_dw = compute_gradient(X, y, w, b)   

        # Update Parameters using w, b, alpha and gradient
        w = w - alpha * dj_dw               
        b = b - alpha * dj_db               
      
        # Save cost J at each iteration
        jList.append( compute_cost(X, y, w, b) )

        # Print cost in interval of 1000 iteration
        if i% math.ceil(iters / 10) == 0:
            print(f"Iteration {i:4d}: Cost {jList[-1]}   ")
        
    return w, b, jList         #return final w,b and J history for graphing

def predict(X, w, b): 
      
    m, n = X.shape   
    yPred = np.zeros(m)
   
    for i in range(m):   
        z_wb = np.dot(X[i],w) 
        z_wb += b
        f_wb = sigmoid(z_wb)

        # Apply the threshold
        yPred[i] = 1 if f_wb>=0.5 else 0
    
    return yPred

def accuracy(yPred, yTrue):
  return 1- np.sum(np.abs(yPred-yTrue))/len(yPred)

w_tmp  = np.zeros_like(x_train[0])
b_tmp  = 0.
alph = 0.1
iters = 10000
lambda_ = 1

w_out, b_out, J_history = gradient_descent(x_train, y_train, w_tmp, b_tmp, alph, iters) 
print(f"\nupdated parameters: w:{w_out}, b:{b_out}")

# accuracy of test set
normalizedInput = (x_test - X_mu)/X_sigma

yPred = predict(normalizedInput, w_out, b_out)
acc = accuracy(yPred, y_test)
print(f"accuracy of test set = {acc}")

# accuracy of training set
yPred = predict(x_train, w_out, b_out)
acc = accuracy(yPred, y_train)
print(f"accuracy of training set = {acc}")

w_tmp  = np.zeros_like(x_train[0])
b_tmp  = 0.
alph = 0.01
iters = 10000
lambda_ = 1

w_out, b_out, J_history = gradient_descent(x_train, y_train, w_tmp, b_tmp, alph, iters) 
print(f"\nupdated parameters: w:{w_out}, b:{b_out}")

# accuracy of test set
normalizedInput = (x_test - X_mu)/X_sigma
yPred = predict(normalizedInput, w_out, b_out)
acc = accuracy(yPred, y_test)
print(f"accuracy of test set = {acc}")


# accuracy of training set
yPred = predict(x_train, w_out, b_out)
acc = accuracy(yPred, y_train)
print(f"accuracy of training set = {acc}")

fig, ax1 = plot.subplots(1, 1, constrained_layout=True, figsize=(12,4))
ax1.plot(J_history)
plot.show()

w_tmp  = np.zeros_like(x_train[0])
b_tmp  = 0.
alph = 0.001
iters = 10000


w_out, b_out, J_history = gradient_descent(x_train, y_train, w_tmp, b_tmp, alph, iters) 
print(f"\nupdated parameters: w:{w_out}, b:{b_out}")

# accuracy of test set
normalizedInput = (x_test - X_mu)/X_sigma
yPred = predict(normalizedInput, w_out, b_out)
acc = accuracy(yPred, y_test)
print(f"accuracy of test set = {acc}")


# accuracy of training set
yPred = predict(x_train, w_out, b_out)
acc = accuracy(yPred, y_train)
print(f"accuracy of training set = {acc}")

w_tmp  = np.zeros_like(x_train[0])
b_tmp  = 0.
alph = 0.0001
iters = 10000


w_out, b_out, J_history = gradient_descent(x_train, y_train, w_tmp, b_tmp, alph, iters) 
print(f"\nupdated parameters: w:{w_out}, b:{b_out}")

# accuracy of test set
normalizedInput = (x_test - X_mu)/X_sigma
yPred = predict(normalizedInput, w_out, b_out)
acc = accuracy(yPred, y_test)
print(f"accuracy of test set = {acc}")


# accuracy of training set
yPred = predict(x_train, w_out, b_out)
acc = accuracy(yPred, y_train)
print(f"accuracy of training set = {acc}")

w_tmp  = np.zeros_like(x_train[0])
b_tmp  = 0.
alph = 30
iters = 10000


w_out, b_out, J_history = gradient_descent(x_train, y_train, w_tmp, b_tmp, alph, iters) 
print(f"\nupdated parameters: w:{w_out}, b:{b_out}")

# accuracy of test set
normalizedInput = (x_test - X_mu)/X_sigma
yPred = predict(normalizedInput, w_out, b_out)
acc = accuracy(yPred, y_test)
print(f"accuracy of test set = {acc}")


# accuracy of training set
yPred = predict(x_train, w_out, b_out)
acc = accuracy(yPred, y_train)
print(f"accuracy of training set = {acc}")

